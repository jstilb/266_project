{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0146f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 10:04:18.362953: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-29 10:04:18.563329: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-29 10:04:19.274604: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jstil/miniconda3/envs/tf/lib/\n",
      "2022-10-29 10:04:19.276950: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jstil/miniconda3/envs/tf/lib/\n",
      "2022-10-29 10:04:19.276968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import DebertaTokenizer, TFDebertaForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# These auto classes load the right type of tokenizer and model based on a model name\n",
    "from transformers import AutoTokenizer, TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d96baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, TF uses python ramdom and numpy library, so these must also be fixed\n",
    "tf.random.set_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7083eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_media_cloud_data(path, label):\n",
    "    \"\"\"Read in data downloaded from media cloud and assign a label to all rows\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df['Label_bias'] = label\n",
    "    df = df.rename({'title': 'sentence'}, axis=1)\n",
    "    return df\n",
    "\n",
    "# read in two datasets\n",
    "PATH_biased = \"data/news_headlines_usa_biased.csv\"\n",
    "PATH_neutral = \"data/news_headlines_usa_neutral.csv\"\n",
    "df_biased = read_media_cloud_data(PATH_biased, 1)\n",
    "df_neutral = read_media_cloud_data(PATH_neutral, 0)\n",
    "\n",
    "# combine them\n",
    "df_distant = pd.concat([df_biased,df_neutral], axis=0, ignore_index=1)\n",
    "df_distant = shuffle(df_distant)\n",
    "\n",
    "# train-test split\n",
    "df_distant_train, df_distant_test = train_test_split(df_distant, test_size=0.2)\n",
    "\n",
    "# # test pipeline set\n",
    "# df_distant, exclude = train_test_split(df_distant, test_size=0.95)\n",
    "# df_distant_train, df_distant_test = train_test_split(df_distant, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a2682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, model_name):\n",
    "    \"\"\"convert a pandas dataframe into a tensorflow dataset\"\"\"\n",
    "    df2 = df.copy(deep=False)\n",
    "    target = df2.pop('Label_bias')\n",
    "    sentence = df2.pop('sentence')\n",
    "\n",
    "    if model_name=='bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_name=='roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    elif model_name=='deberta':\n",
    "        tokenizer = DebertaTokenizer.from_pretrained(\"kamalkraj/deberta-base\")\n",
    "\n",
    "    train_encodings = tokenizer(\n",
    "                        sentence.tolist(),                      \n",
    "                        add_special_tokens = True, # add [CLS], [SEP]\n",
    "                        truncation = True, # cut off at max length of the text that can go to BERT\n",
    "                        padding = True, # add [PAD] tokens\n",
    "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (dict(train_encodings), \n",
    "         target.tolist()))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f54f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_df, test_df, model_name):\n",
    "    # pandas -> tensorflow\n",
    "    train_distant_dataset = preprocess(train_df, model_name)\n",
    "    test_distant_dataset = preprocess(test_df, model_name)\n",
    "\n",
    "    # batch and randomize\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 24\n",
    "\n",
    "    train_distant_dataset = train_distant_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    test_distant_dataset = test_distant_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True) # after 3 epochs without improvement, stop training\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "    \n",
    "    if model_name=='bert':\n",
    "        clf = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "    elif model_name=='roberta':\n",
    "        clf = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "    elif model_name == 'deberta':\n",
    "        clf = TFDebertaForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n",
    "\n",
    "    clf.compile(optimizer=optimizer, loss='binary_crossentropy', metrics='accuracy') \n",
    "    history = clf.fit(train_distant_dataset, epochs=5, validation_data = test_distant_dataset, callbacks=[callback])\n",
    "    trained_layer = clf.get_layer(index=0).get_weights()\n",
    "    clf.save_weights(f'./checkpoints/{model_name}_final_checkpoint_news_headlines_USA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1870bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 10:04:55.844302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 10:04:55.883151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 10:04:55.883719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 10:04:55.884642: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-29 10:04:55.886622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 10:04:55.887188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 10:04:55.887720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 10:04:57.952882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 10:04:57.953590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 10:04:57.953605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-10-29 10:04:57.954172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 10:04:57.954257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5799 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-10-29 10:04:57.956515: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28015456 exceeds 10% of free system memory.\n",
      "2022-10-29 10:05:19.263603: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28015456 exceeds 10% of free system memory.\n",
      "2022-10-29 10:05:27.628082: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28015456 exceeds 10% of free system memory.\n",
      "2022-10-29 10:05:54.040248: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 10:05:55.772237: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28015456 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4292/4292 [==============================] - 2124s 491ms/step - loss: 0.5313 - accuracy: 0.5002 - val_loss: 0.4790 - val_accuracy: 0.7403\n",
      "Epoch 2/5\n",
      "4292/4292 [==============================] - 2174s 507ms/step - loss: 1.4533 - accuracy: 0.4151 - val_loss: 0.4947 - val_accuracy: 0.2266\n"
     ]
    }
   ],
   "source": [
    "# train bert\n",
    "train_model(df_distant_train, df_distant_test, 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed9b1029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4292/4292 [==============================] - 2197s 508ms/step - loss: 2.7369 - accuracy: 0.3803 - val_loss: 3.0565 - val_accuracy: 0.3543\n",
      "Epoch 2/5\n",
      "4292/4292 [==============================] - 2284s 532ms/step - loss: 4.8408 - accuracy: 0.5199 - val_loss: 5.4643 - val_accuracy: 0.6457\n"
     ]
    }
   ],
   "source": [
    "# train roberta\n",
    "train_model(df_distant_train, df_distant_test, 'roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fb94da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'pooler', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /home/jstil/.local/lib/python3.10/site-packages/transformers/models/deberta/modeling_tf_deberta.py:123: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/distributions/bernoulli.py:86: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "4292/4292 [==============================] - 2977s 687ms/step - loss: 0.6555 - accuracy: 0.4432 - val_loss: 0.8204 - val_accuracy: 0.1122\n",
      "Epoch 2/5\n",
      "4292/4292 [==============================] - 2643s 616ms/step - loss: 0.5971 - accuracy: 0.3834 - val_loss: 0.4007 - val_accuracy: 0.1815\n",
      "Epoch 3/5\n",
      "4292/4292 [==============================] - 2677s 624ms/step - loss: 0.3860 - accuracy: 0.3459 - val_loss: 0.4941 - val_accuracy: 0.3543\n"
     ]
    }
   ],
   "source": [
    "# train deberta\n",
    "train_model(df_distant_train, df_distant_test, 'deberta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
