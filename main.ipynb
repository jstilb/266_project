{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17d087bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/jstil/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Sat Oct 29 10:02:46 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 522.25       CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   60C    P8    16W /  N/A |   6889MiB /  8192MiB |      8%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     27636      C   /python3.10                     N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952491bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 09:47:11.174183: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-29 09:47:11.396973: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-29 09:47:11.977983: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jstil/miniconda3/envs/tf/lib/\n",
      "2022-10-29 09:47:11.979972: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jstil/miniconda3/envs/tf/lib/\n",
      "2022-10-29 09:47:11.980005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from typing import Dict, List, Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, BertConfig, TFBertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "from transformers import ElectraTokenizer, TFElectraForSequenceClassification\n",
    "from transformers import XLNetTokenizer, TFXLNetForSequenceClassification\n",
    "from transformers import LongformerTokenizer, TFLongformerForSequenceClassification\n",
    "from transformers import DebertaTokenizer, TFDebertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf0bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, TF uses python ramdom and numpy library, so these must also be fixed\n",
    "tf.random.set_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b41841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if hardware accelerator available\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de2780fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>news_link</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>Label_bias</th>\n",
       "      <th>label_opinion</th>\n",
       "      <th>biased_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Republican president assumed he was helpin...</td>\n",
       "      <td>http://www.msnbc.com/rachel-maddow-show/auto-i...</td>\n",
       "      <td>msnbc</td>\n",
       "      <td>environment</td>\n",
       "      <td>left</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Expresses writer’s opinion</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Though the indictment of a woman for her own p...</td>\n",
       "      <td>https://eu.usatoday.com/story/news/nation/2019...</td>\n",
       "      <td>usa-today</td>\n",
       "      <td>abortion</td>\n",
       "      <td>center</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ingraham began the exchange by noting American...</td>\n",
       "      <td>https://www.breitbart.com/economy/2020/01/12/d...</td>\n",
       "      <td>breitbart</td>\n",
       "      <td>immigration</td>\n",
       "      <td>right</td>\n",
       "      <td>No agreement</td>\n",
       "      <td>No agreement</td>\n",
       "      <td>['flood']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The tragedy of America’s 18 years in Afghanist...</td>\n",
       "      <td>http://feedproxy.google.com/~r/breitbart/~3/ER...</td>\n",
       "      <td>breitbart</td>\n",
       "      <td>international-politics-and-world-news</td>\n",
       "      <td>right</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>['tragedy', 'stubborn']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The justices threw out a challenge from gun ri...</td>\n",
       "      <td>https://www.huffpost.com/entry/supreme-court-g...</td>\n",
       "      <td>msnbc</td>\n",
       "      <td>gun-control</td>\n",
       "      <td>left</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>Entirely factual</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  The Republican president assumed he was helpin...   \n",
       "1  Though the indictment of a woman for her own p...   \n",
       "2  Ingraham began the exchange by noting American...   \n",
       "3  The tragedy of America’s 18 years in Afghanist...   \n",
       "4  The justices threw out a challenge from gun ri...   \n",
       "\n",
       "                                           news_link     outlet  \\\n",
       "0  http://www.msnbc.com/rachel-maddow-show/auto-i...      msnbc   \n",
       "1  https://eu.usatoday.com/story/news/nation/2019...  usa-today   \n",
       "2  https://www.breitbart.com/economy/2020/01/12/d...  breitbart   \n",
       "3  http://feedproxy.google.com/~r/breitbart/~3/ER...  breitbart   \n",
       "4  https://www.huffpost.com/entry/supreme-court-g...      msnbc   \n",
       "\n",
       "                                   topic    type    Label_bias  \\\n",
       "0                            environment    left        Biased   \n",
       "1                               abortion  center    Non-biased   \n",
       "2                            immigration   right  No agreement   \n",
       "3  international-politics-and-world-news   right        Biased   \n",
       "4                            gun-control    left    Non-biased   \n",
       "\n",
       "                           label_opinion             biased_words  \n",
       "0             Expresses writer’s opinion                       []  \n",
       "1  Somewhat factual but also opinionated                       []  \n",
       "2                           No agreement                ['flood']  \n",
       "3  Somewhat factual but also opinionated  ['tragedy', 'stubborn']  \n",
       "4                       Entirely factual                       []  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_sg1 = \"data/final_labels_SG1.xlsx\"\n",
    "PATH_sg2 = \"data/final_labels_SG2.xlsx\"\n",
    "df_sg1 = pd.read_excel(PATH_sg1)\n",
    "df_sg2 = pd.read_excel(PATH_sg2)\n",
    "df_sg1.rename(columns={'text': 'sentence', 'label_bias': 'Label_bias'}, inplace=True)\n",
    "df_sg2.rename(columns={'text': 'sentence', 'label_bias': 'Label_bias'}, inplace=True)\n",
    "df_sg1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7a7000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize classification problem\n",
    "df_sg1 = df_sg1[df_sg1['Label_bias']!='No agreement']\n",
    "df_sg1 = df_sg1[df_sg1['Label_bias'].isna()==False]\n",
    "df_sg1.replace(to_replace='Biased', value=1, inplace=True)\n",
    "df_sg1.replace(to_replace='Non-biased', value=0, inplace=True)\n",
    "\n",
    "df_sg2 = df_sg2[df_sg2['Label_bias']!='No agreement']\n",
    "df_sg2.replace(to_replace='Biased', value=1, inplace=True)\n",
    "df_sg2.replace(to_replace='Non-biased', value=0, inplace=True)\n",
    "\n",
    "# test pipeline set\n",
    "df_sg1, exclude = train_test_split(df_sg1, test_size=0.95)\n",
    "df_sg2, exclude = train_test_split(df_sg2, test_size=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d5fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified k-Fold instance\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4e37878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions called in skfold loop\n",
    "\n",
    "def pd_to_tf(df):\n",
    "    \"\"\"convert a pandas dataframe into a tensorflow dataset\"\"\"\n",
    "    target = df.pop('Label_bias')\n",
    "    sentence = df.pop('sentence')\n",
    "    return tf.data.Dataset.from_tensor_slices((sentence.values, target.values))\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "def tokenize(df, model_name):\n",
    "    \"\"\"convert a pandas dataframe into a tensorflow dataset and run hugging face's tokenizer on data\"\"\"\n",
    "    df2 = df.copy(deep=False)\n",
    "    target = df2.pop('Label_bias')\n",
    "    sentence = df2.pop('sentence')\n",
    "    \n",
    "    if model_name=='bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_name=='roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    elif model_name=='deberta':\n",
    "        tokenizer = DebertaTokenizer.from_pretrained(\"kamalkraj/deberta-base\")\n",
    "    elif model_name=='distilbert':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    elif model_name=='electra':\n",
    "        tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "    elif model_name=='xlnet':\n",
    "        tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "    train_encodings = tokenizer(\n",
    "                        sentence.tolist(),                      \n",
    "                        add_special_tokens = True, # add [CLS], [SEP]\n",
    "                        truncation = True, # cut off at max length of the text that can go to BERT\n",
    "                        padding = True, # add [PAD] tokens\n",
    "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (dict(train_encodings), \n",
    "         target.tolist()))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb710374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_5fold(df_name, df_train, model_name, freeze_encoder=False, pretrained=False, plot=False):\n",
    "    \"\"\"\"freeze flags whether encoder layer should be frozen to not destroy transfer learning. Only set to false when enough data is provided\"\"\"\n",
    "\n",
    "    # these variables will be needed for skfold to select indices\n",
    "    Y = df_train['Label_bias']\n",
    "    X = df_train['sentence']\n",
    "\n",
    "    # hyperparams\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 12\n",
    "    k = 1\n",
    "\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_f1 = []\n",
    "    val_f1_micro = []\n",
    "    val_f1_wmacro = []\n",
    "    \n",
    "    if pretrained==True:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "        \n",
    "        if model_name=='bert':\n",
    "            transfer_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "            \n",
    "        elif model_name=='roberta':\n",
    "            transfer_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "            \n",
    "        elif model_name=='deberta':\n",
    "            transfer_model = TFDebertaForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n",
    "            \n",
    "        transfer_model.compile(optimizer=optimizer, loss='binary_crossentropy') \n",
    "        transfer_model.load_weights(f'./checkpoints/{model_name}_final_checkpoint_news_headlines_USA')\n",
    "        trained_model_layer = transfer_model.get_layer(index=0).get_weights()\n",
    "            \n",
    "\n",
    "    for train_index, val_index in skfold.split(X,Y):\n",
    "        print('### Start fold {}'.format(k))\n",
    "\n",
    "        # split into train and validation set\n",
    "        train_dataset = df_train.iloc[train_index]\n",
    "        val_dataset = df_train.iloc[val_index]\n",
    "\n",
    "        # prepare data for transformer\n",
    "        train_dataset = tokenize(train_dataset, model_name)\n",
    "        val_dataset = tokenize(val_dataset, model_name)\n",
    "\n",
    "        # mini-batch it\n",
    "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        # create new model\n",
    "        if model_name == 'bert':\n",
    "            model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "        if model_name == 'distilbert':\n",
    "            model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "        elif model_name == 'roberta':\n",
    "            model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "        elif model_name == 'electra':\n",
    "            model = TFElectraForSequenceClassification.from_pretrained('google/electra-small-discriminator')\n",
    "        elif model_name == 'xlnet':\n",
    "            model = TFXLNetForSequenceClassification.from_pretrained('xlnet-base-cased')\n",
    "        elif model_name == 'deberta':\n",
    "            model = TFDebertaForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n",
    "\n",
    "\n",
    "        if freeze_encoder == True:\n",
    "            for w in model.get_layer(index=0).weights:\n",
    "                w._trainable = False\n",
    "\n",
    "        # compile it\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy') \n",
    "\n",
    "        # transfer learning\n",
    "        if pretrained == True:\n",
    "            model.get_layer(index=0).set_weights(trained_model_layer) # load bias-specific weights\n",
    "\n",
    "        # after 2 epochs without improvement, stop training\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "\n",
    "        # fit it\n",
    "        history = model.fit(train_dataset, epochs=1, validation_data = val_dataset, callbacks=[callback])\n",
    "\n",
    "        # plot history\n",
    "        if plot:\n",
    "            plot_graphs(history,'loss')\n",
    "\n",
    "        # evaluate\n",
    "        loss = model.evaluate(val_dataset)\n",
    "\n",
    "        if model_name == 'xlnet':\n",
    "            tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "            yhats = []\n",
    "            for row in df_train.iloc[val_index]['sentence']:\n",
    "                input = tokenizer(row, return_tensors=\"tf\")\n",
    "                output = model(input)\n",
    "                logits = output.logits.numpy()[0]\n",
    "                candidates = logits.tolist()\n",
    "                decision = candidates.index(max(candidates))\n",
    "                yhats.append(decision)\n",
    "        else:\n",
    "            logits = model.predict(val_dataset)  \n",
    "            yhats = []\n",
    "            for i in logits[0]:\n",
    "                # assign class label according to highest logit\n",
    "                candidates = i.tolist()\n",
    "                decision = candidates.index(max(candidates))\n",
    "                yhats.append(decision)\n",
    "\n",
    "        y = []\n",
    "        for text, label in val_dataset.unbatch():   \n",
    "            y.append(label.numpy())\n",
    "\n",
    "        val_loss.append(loss)\n",
    "        val_acc.append(accuracy_score(y, yhats))\n",
    "        val_prec.append(precision_score(y, yhats))\n",
    "        val_rec.append(recall_score(y, yhats))\n",
    "        val_f1.append(f1_score(y, yhats))\n",
    "        val_f1_micro.append(f1_score(y, yhats, average='micro'))\n",
    "        val_f1_wmacro.append(f1_score(y, yhats, average='weighted'))\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        k += 1\n",
    "\n",
    "        return {'loss': val_loss, 'acc': val_acc, 'prec': val_prec, 'rec': val_rec, 'f1': val_f1, \n",
    "                'f1_micro': val_f1_micro, 'f1_wmacro': val_f1_wmacro, 'model_name': model_name, \n",
    "                'distant': pretrained, 'df_name': df_name} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7efd6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(d, results):\n",
    "    loss_cv = np.mean(d['loss'])\n",
    "    acc_cv = np.mean(d['acc'])\n",
    "    prec_cv = np.mean(d['prec'])\n",
    "    rec_cv = np.mean(d['rec'])\n",
    "    f1_cv = np.mean(d['f1'])\n",
    "    f1_micro_cv = np.mean(d['f1_micro'])\n",
    "    f1_wmacro_cv = np.mean(d['f1_wmacro'])\n",
    "    \n",
    "    row = {\n",
    "        'Dataset': d['df_name'],\n",
    "        'Model': d['model_name'], \n",
    "        'Distant': d['distant'], \n",
    "        'Loss': loss_cv, \n",
    "        'Accuracy': acc_cv, \n",
    "        'Precision': prec_cv, \n",
    "        'Recall': rec_cv, \n",
    "        'F1': f1_cv, \n",
    "        'F1 Micro': f1_micro_cv, \n",
    "        'F1 Weighted': f1_wmacro_cv\n",
    "    }\n",
    "    \n",
    "    results = results.append(row, ignore_index=True)\n",
    "    print(row)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a70f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "## instantiate results df\n",
    "columns = ['Dataset', 'Model', 'Distant', 'Loss', 'Accuracy', 'Precision', 'Recall', 'F1', 'F1 Micro', 'F1 Weighted']\n",
    "results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d34f6",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'bert', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e445144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'bert', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d083b",
   "metadata": {},
   "source": [
    "## Bert w/ Distant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d557ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'bert', freeze_encoder=False, pretrained=True, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'bert', freeze_encoder=False, pretrained=True, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f92b4",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8420cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'roberta', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a6aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'roberta', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd8dcca",
   "metadata": {},
   "source": [
    "## Roberta w/ Distant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aacaf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'roberta', freeze_encoder=False, pretrained=True, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53bd4247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 23s 550ms/step - loss: 2.2121 - val_loss: 0.7697\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.7697\n",
      "4/4 [==============================] - 3s 82ms/step\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "{'Dataset': 'sg2', 'Model': 'roberta', 'Distant': True, 'Loss': 0.769734799861908, 'Accuracy': 0.4594594594594595, 'Precision': 0.4444444444444444, 'Recall': 1.0, 'F1': 0.6153846153846153, 'F1 Micro': 0.4594594594594595, 'F1 Weighted': 0.31770931770931765}\n",
      "  Dataset    Model Distant      Loss  Accuracy  Precision  Recall        F1  \\\n",
      "0     sg1  deberta    True  4.138522  0.500000   0.000000     0.0  0.000000   \n",
      "1     sg2  roberta    True  0.769735  0.459459   0.444444     1.0  0.615385   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.500000     0.333333  \n",
      "1  0.459459     0.317709  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27636/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_27636/70255357.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'roberta', freeze_encoder=False, pretrained=True, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220cd051",
   "metadata": {},
   "source": [
    "## Deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a684ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'deberta', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb2a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'deberta', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeca876",
   "metadata": {},
   "source": [
    "## Deberta w/ Distant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bb34cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 09:48:02.459631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 09:48:02.482439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 09:48:02.483003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 09:48:02.484292: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-29 09:48:02.488286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 09:48:02.488837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 09:48:02.489305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 09:48:03.373483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 09:48:03.374109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 09:48:03.374125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-10-29 09:48:03.374685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-29 09:48:03.374804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'pooler', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'pooler', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jstil/.local/lib/python3.10/site-packages/transformers/models/deberta/modeling_tf_deberta.py:123: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/distributions/bernoulli.py:86: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "6/6 [==============================] - 40s 2s/step - loss: 5.2666 - val_loss: 4.1385\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 4.1385\n",
      "2/2 [==============================] - 6s 76ms/step\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "{'Dataset': 'sg1', 'Model': 'deberta', 'Distant': True, 'Loss': 4.138521671295166, 'Accuracy': 0.5, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'F1 Micro': 0.5, 'F1 Weighted': 0.3333333333333333}\n",
      "  Dataset    Model Distant      Loss  Accuracy  Precision  Recall   F1  \\\n",
      "0     sg1  deberta    True  4.138522       0.5        0.0     0.0  0.0   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0       0.5     0.333333  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/tmp/ipykernel_27636/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'deberta', freeze_encoder=False, pretrained=True, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4c176ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'pooler', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'pooler', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 40s 1s/step - loss: 2.3796 - val_loss: 0.7459\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.7459\n",
      "4/4 [==============================] - 5s 91ms/step\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "{'Dataset': 'sg2', 'Model': 'deberta', 'Distant': True, 'Loss': 0.7459329962730408, 'Accuracy': 0.43243243243243246, 'Precision': 0.43243243243243246, 'Recall': 1.0, 'F1': 0.6037735849056604, 'F1 Micro': 0.43243243243243246, 'F1 Weighted': 0.2610912799592045}\n",
      "  Dataset    Model Distant      Loss  Accuracy  Precision  Recall        F1  \\\n",
      "0     sg1  deberta    True  4.138522  0.500000   0.000000     0.0  0.000000   \n",
      "1     sg2  roberta    True  0.769735  0.459459   0.444444     1.0  0.615385   \n",
      "2     sg2  deberta    True  0.745933  0.432432   0.432432     1.0  0.603774   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.500000     0.333333  \n",
      "1  0.459459     0.317709  \n",
      "2  0.432432     0.261091  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27636/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_27636/70255357.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'deberta', freeze_encoder=False, pretrained=True, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318ca89",
   "metadata": {},
   "source": [
    "## Distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3883467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2690fc5c04a4ed891040f2c4d5e8baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0654f6daf688441e9ccf9655ef701d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076844c9d23f4bef81e37ed931362f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f701ef9e1454496580763f63792f1e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/363M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 424ms/step - loss: 1.6819 - val_loss: 0.8514\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.8514\n",
      "2/2 [==============================] - 1s 34ms/step\n",
      "{'Dataset': 'sg1', 'Model': 'distilbert', 'Distant': False, 'Loss': 0.8513896465301514, 'Accuracy': 0.5, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'F1 Micro': 0.5, 'F1 Weighted': 0.3333333333333333}\n",
      "  Dataset       Model Distant      Loss  Accuracy  Precision  Recall  \\\n",
      "0     sg1     deberta    True  4.138522  0.500000   0.000000    0.00   \n",
      "1     sg2     roberta    True  0.769735  0.459459   0.444444    1.00   \n",
      "2     sg2     deberta    True  0.745933  0.432432   0.432432    1.00   \n",
      "3     sg1     electra   False  1.042484  0.437500   0.461538    0.75   \n",
      "4     sg1       xlnet   False  3.889588  0.500000   0.500000    1.00   \n",
      "5     sg1  distilbert   False  0.851390  0.500000   0.000000    0.00   \n",
      "\n",
      "         F1  F1 Micro  F1 Weighted  \n",
      "0  0.000000  0.500000     0.333333  \n",
      "1  0.615385  0.459459     0.317709  \n",
      "2  0.603774  0.432432     0.261091  \n",
      "3  0.571429  0.437500     0.376623  \n",
      "4  0.666667  0.500000     0.333333  \n",
      "5  0.000000  0.500000     0.333333  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/tmp/ipykernel_27636/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_27636/70255357.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'distilbert', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f067395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'distilbert', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920adc2e",
   "metadata": {},
   "source": [
    "## Xlnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4e5a107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification_1/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification_1/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "6/6 [==============================] - 16s 830ms/step - loss: 2.5299 - val_loss: 3.8896\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 3.8896\n",
      "{'Dataset': 'sg1', 'Model': 'xlnet', 'Distant': False, 'Loss': 3.8895883560180664, 'Accuracy': 0.5, 'Precision': 0.5, 'Recall': 1.0, 'F1': 0.6666666666666666, 'F1 Micro': 0.5, 'F1 Weighted': 0.3333333333333333}\n",
      "  Dataset    Model Distant      Loss  Accuracy  Precision  Recall        F1  \\\n",
      "0     sg1  deberta    True  4.138522  0.500000   0.000000    0.00  0.000000   \n",
      "1     sg2  roberta    True  0.769735  0.459459   0.444444    1.00  0.615385   \n",
      "2     sg2  deberta    True  0.745933  0.432432   0.432432    1.00  0.603774   \n",
      "3     sg1  electra   False  1.042484  0.437500   0.461538    0.75  0.571429   \n",
      "4     sg1    xlnet   False  3.889588  0.500000   0.500000    1.00  0.666667   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.500000     0.333333  \n",
      "1  0.459459     0.317709  \n",
      "2  0.432432     0.261091  \n",
      "3  0.437500     0.376623  \n",
      "4  0.500000     0.333333  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27636/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_27636/70255357.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'xlnet', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64444ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'xlnet', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477bc36",
   "metadata": {},
   "source": [
    "## Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02019c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c069312bd54694ae317112b8d0d7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11196178ded844b7a1369d9cd31acfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78a6d4fa60647dfb75da096913c5984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953f6402070447a6a667637fbd9a730b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/54.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 19s 652ms/step - loss: 1.9600 - val_loss: 1.0425\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 1.0425\n",
      "2/2 [==============================] - 3s 104ms/step\n",
      "{'Dataset': 'sg1', 'Model': 'electra', 'Distant': False, 'Loss': 1.0424840450286865, 'Accuracy': 0.4375, 'Precision': 0.46153846153846156, 'Recall': 0.75, 'F1': 0.5714285714285714, 'F1 Micro': 0.4375, 'F1 Weighted': 0.37662337662337664}\n",
      "  Dataset    Model Distant      Loss  Accuracy  Precision  Recall        F1  \\\n",
      "0     sg1  deberta    True  4.138522  0.500000   0.000000    0.00  0.000000   \n",
      "1     sg2  roberta    True  0.769735  0.459459   0.444444    1.00  0.615385   \n",
      "2     sg2  deberta    True  0.745933  0.432432   0.432432    1.00  0.603774   \n",
      "3     sg1  electra   False  1.042484  0.437500   0.461538    0.75  0.571429   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.500000     0.333333  \n",
      "1  0.459459     0.317709  \n",
      "2  0.432432     0.261091  \n",
      "3  0.437500     0.376623  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27636/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_27636/70255357.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'electra', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'electra', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
