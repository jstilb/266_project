{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c98004b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/jstil/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/jstil/.local/lib/python3.10/site-packages (4.23.1)\n",
      "Requirement already satisfied: filelock in /home/jstil/.local/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jstil/.local/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jstil/.local/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jstil/.local/lib/python3.10/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/jstil/.local/lib/python3.10/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jstil/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.17.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jstil/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "/bin/bash: /home/jstil/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/jstil/.local/lib/python3.10/site-packages (0.1.97)\n",
      "/bin/bash: /home/jstil/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in /home/jstil/.local/lib/python3.10/site-packages (3.0.10)\n",
      "Requirement already satisfied: et-xmlfile in /home/jstil/.local/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d087bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/jstil/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Sun Nov  6 16:27:14 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 522.25       CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   57C    P8    17W /  N/A |    577MiB /  8192MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "952491bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 16:27:20.272186: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-06 16:27:20.883361: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-06 16:27:21.660430: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jstil/miniconda3/envs/tf/lib/\n",
      "2022-11-06 16:27:21.662132: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jstil/miniconda3/envs/tf/lib/\n",
      "2022-11-06 16:27:21.662142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from typing import Dict, List, Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, BertConfig, TFBertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "from transformers import ElectraTokenizer, TFElectraForSequenceClassification\n",
    "from transformers import XLNetTokenizer, TFXLNetForSequenceClassification\n",
    "from transformers import LongformerTokenizer, TFLongformerForSequenceClassification\n",
    "from transformers import DebertaTokenizer, TFDebertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf0bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, TF uses python ramdom and numpy library, so these must also be fixed\n",
    "tf.random.set_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b41841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 16:27:23.010353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:23.074746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:23.075510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see if hardware accelerator available\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d1e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 16:27:23.093531: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-06 16:27:23.098450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:23.099035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:23.099567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:24.366427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:24.367420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not op"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "en file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:24.367441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-06 16:27:24.368080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:24.368273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de2780fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_sg1 = \"data/final_labels_SG1.xlsx\"\n",
    "PATH_sg2 = \"data/final_labels_SG2.xlsx\"\n",
    "# df_sg1 = pd.read_excel(PATH_sg1)\n",
    "df_sg2 = pd.read_excel(PATH_sg2)\n",
    "#df_sg1.rename(columns={'text': 'sentence', 'label_bias': 'Label_bias'}, inplace=True)\n",
    "df_sg2.rename(columns={'text': 'sentence', 'label_bias': 'Label_bias'}, inplace=True)\n",
    "# df_sg1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a7000e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3829"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binarize classification problem\n",
    "# df_sg1 = df_sg1[df_sg1['Label_bias']!='No agreement']\n",
    "# df_sg1 = df_sg1[df_sg1['Label_bias'].isna()==False]\n",
    "# df_sg1.replace(to_replace='Biased', value=1, inplace=True)\n",
    "# df_sg1.replace(to_replace='Non-biased', value=0, inplace=True)\n",
    "\n",
    "df_sg2 = df_sg2[df_sg2['Label_bias']!='No agreement']\n",
    "df_sg2.replace(to_replace='Biased', value=1, inplace=True)\n",
    "df_sg2.replace(to_replace='Non-biased', value=0, inplace=True)\n",
    "\n",
    "# # test pipeline set\n",
    "# df_sg1, exclude = train_test_split(df_sg1, test_size=0.95)\n",
    "# df_sg2, exclude = train_test_split(df_sg2, test_size=0.8)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d5fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified k-Fold instance\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4e37878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions called in skfold loop\n",
    "\n",
    "def pd_to_tf(df):\n",
    "    \"\"\"convert a pandas dataframe into a tensorflow dataset\"\"\"\n",
    "    target = df.pop('Label_bias')\n",
    "    sentence = df.pop('sentence')\n",
    "    return tf.data.Dataset.from_tensor_slices((sentence.values, target.values))\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "def tokenize(df, model_name):\n",
    "    \"\"\"convert a pandas dataframe into a tensorflow dataset and run hugging face's tokenizer on data\"\"\"\n",
    "    df2 = df.copy(deep=False)\n",
    "    target = df2.pop('Label_bias')\n",
    "    sentence = df2.pop('sentence')\n",
    "    \n",
    "    if model_name=='bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_name=='roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    elif model_name=='deberta':\n",
    "        tokenizer = DebertaTokenizer.from_pretrained(\"kamalkraj/deberta-base\")\n",
    "    elif model_name=='distilbert':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    elif model_name=='electra':\n",
    "        tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "    elif model_name=='xlnet':\n",
    "        tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "    train_encodings = tokenizer(\n",
    "                        sentence.tolist(),                      \n",
    "                        add_special_tokens = True, # add [CLS], [SEP]\n",
    "                        truncation = True, # cut off at max length of the text that can go to BERT\n",
    "                        padding = True, # add [PAD] tokens\n",
    "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (dict(train_encodings), \n",
    "         target.tolist()))\n",
    "    \n",
    "    # clear unused memory\n",
    "    del(df2)\n",
    "    del(target)\n",
    "    del(sentence)\n",
    "    del(tokenizer)\n",
    "    del(train_encodings)\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb710374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_5fold(df_name, df_train, model_name, freeze_encoder=False, pretrained=False, plot=False, batch_size=12, epochs=1):\n",
    "    \"\"\"\"freeze flags whether encoder layer should be frozen to not destroy transfer learning. Only set to false when enough data is provided\"\"\"\n",
    "\n",
    "    # these variables will be needed for skfold to select indices\n",
    "    Y = df_train['Label_bias']\n",
    "    X = df_train['sentence']\n",
    "\n",
    "    # hyperparams\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = batch_size\n",
    "    k = 1\n",
    "\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    val_prec = []\n",
    "    val_rec = []\n",
    "    val_f1 = []\n",
    "    val_f1_micro = []\n",
    "    val_f1_wmacro = []\n",
    "    \n",
    "    if pretrained==True:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "        \n",
    "        if model_name=='bert':\n",
    "            transfer_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "            \n",
    "        elif model_name=='roberta':\n",
    "            transfer_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "            \n",
    "        elif model_name=='deberta':\n",
    "            transfer_model = TFDebertaForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n",
    "            \n",
    "        transfer_model.compile(optimizer=optimizer, loss='binary_crossentropy') \n",
    "        transfer_model.load_weights(f'./checkpoints/{model_name}_final_checkpoint_news_headlines_USA')\n",
    "        trained_model_layer = transfer_model.get_layer(index=0).get_weights()\n",
    "            \n",
    "\n",
    "    for train_index, val_index in skfold.split(X,Y):\n",
    "        print('### Start fold {}'.format(k))\n",
    "\n",
    "        # split into train and validation set\n",
    "        train_dataset = df_train.iloc[train_index]\n",
    "        val_dataset = df_train.iloc[val_index]\n",
    "\n",
    "        # prepare data for transformer\n",
    "        train_dataset = tokenize(train_dataset, model_name)\n",
    "        val_dataset = tokenize(val_dataset, model_name)\n",
    "\n",
    "        # mini-batch it\n",
    "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        # create new model\n",
    "        if model_name == 'bert':\n",
    "            model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "        if model_name == 'distilbert':\n",
    "            model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "        elif model_name == 'roberta':\n",
    "            model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "        elif model_name == 'electra':\n",
    "            model = TFElectraForSequenceClassification.from_pretrained('google/electra-small-discriminator')\n",
    "        elif model_name == 'xlnet':\n",
    "            model = TFXLNetForSequenceClassification.from_pretrained('xlnet-base-cased')\n",
    "        elif model_name == 'deberta':\n",
    "            model = TFDebertaForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n",
    "\n",
    "\n",
    "        if freeze_encoder == True:\n",
    "            for w in model.get_layer(index=0).weights:\n",
    "                w._trainable = False\n",
    "\n",
    "        # compile it\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy') \n",
    "\n",
    "        # transfer learning\n",
    "        if pretrained == True:\n",
    "            model.get_layer(index=0).set_weights(trained_model_layer) # load bias-specific weights\n",
    "\n",
    "        # after 2 epochs without improvement, stop training\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "\n",
    "        # fit it\n",
    "        history = model.fit(train_dataset, epochs=epochs, validation_data = val_dataset, callbacks=[callback])\n",
    "\n",
    "        # plot history\n",
    "        if plot:\n",
    "            plot_graphs(history,'loss')\n",
    "\n",
    "        # evaluate\n",
    "        loss = model.evaluate(val_dataset)\n",
    "\n",
    "        if model_name == 'xlnet':\n",
    "            tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "            yhats = []\n",
    "            for row in df_train.iloc[val_index]['sentence']:\n",
    "                input = tokenizer(row, return_tensors=\"tf\")\n",
    "                output = model(input)\n",
    "                logits = output.logits.numpy()[0]\n",
    "                candidates = logits.tolist()\n",
    "                decision = candidates.index(max(candidates))\n",
    "                yhats.append(decision)\n",
    "        else:\n",
    "            logits = model.predict(val_dataset)  \n",
    "            yhats = []\n",
    "            for i in logits[0]:\n",
    "                # assign class label according to highest logit\n",
    "                candidates = i.tolist()\n",
    "                decision = candidates.index(max(candidates))\n",
    "                yhats.append(decision)\n",
    "\n",
    "        y = []\n",
    "        for text, label in val_dataset.unbatch():   \n",
    "            y.append(label.numpy())\n",
    "        \n",
    "\n",
    "        val_loss.append(loss)\n",
    "        val_acc.append(accuracy_score(y, yhats))\n",
    "        val_prec.append(precision_score(y, yhats))\n",
    "        val_rec.append(recall_score(y, yhats))\n",
    "        val_f1.append(f1_score(y, yhats))\n",
    "        val_f1_micro.append(f1_score(y, yhats, average='micro'))\n",
    "        val_f1_wmacro.append(f1_score(y, yhats, average='weighted'))\n",
    "        \n",
    "#         print(f\"y={y}\")\n",
    "#         print(f\"y^={yhats}\")\n",
    "#         print(f\"prec={val_prec}\")\n",
    "#         print(f\"rec={val_rec}\")\n",
    "#         print(f\"f1={val_f1}\")\n",
    "#         print(f\"f1={val_f1_micro}\")\n",
    "#         print(f\"f1={val_f1_wmacro}\")\n",
    "        \n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # clear unused memory\n",
    "        del(model)\n",
    "        del(train_dataset)\n",
    "        del(val_dataset)\n",
    "        del(history)\n",
    "        del(optimizer)\n",
    "        del(callback)\n",
    "        del(loss)\n",
    "        del(y)\n",
    "        gc.collect()\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    return {'loss': val_loss, 'acc': val_acc, 'prec': val_prec, 'rec': val_rec, 'f1': val_f1, \n",
    "            'f1_micro': val_f1_micro, 'f1_wmacro': val_f1_wmacro, 'model_name': model_name, \n",
    "            'distant': pretrained, 'df_name': df_name} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7efd6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(d, results):\n",
    "    loss_cv = np.mean(d['loss'])\n",
    "    acc_cv = np.mean(d['acc'])\n",
    "    prec_cv = np.mean(d['prec'])\n",
    "    rec_cv = np.mean(d['rec'])\n",
    "    f1_cv = np.mean(d['f1'])\n",
    "    f1_micro_cv = np.mean(d['f1_micro'])\n",
    "    f1_wmacro_cv = np.mean(d['f1_wmacro'])\n",
    "    \n",
    "    row = {\n",
    "        'Dataset': d['df_name'],\n",
    "        'Model': d['model_name'], \n",
    "        'Distant': d['distant'], \n",
    "        'Loss': loss_cv, \n",
    "        'Accuracy': acc_cv, \n",
    "        'Precision': prec_cv, \n",
    "        'Recall': rec_cv, \n",
    "        'F1': f1_cv, \n",
    "        'F1 Micro': f1_micro_cv, \n",
    "        'F1 Weighted': f1_wmacro_cv\n",
    "    }\n",
    "    \n",
    "    results = results.append(row, ignore_index=True)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a70f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "## instantiate results df\n",
    "columns = ['Dataset', 'Model', 'Distant', 'Loss', 'Accuracy', 'Precision', 'Recall', 'F1', 'F1 Micro', 'F1 Weighted']\n",
    "results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88af360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 15:22:55.195831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 15:22:55.196476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 15:22:55.197339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 15:22:55.198145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 15:22:55.198159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-06 15:22:55.198677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 15:22:55.198705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'classifier', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'classifier', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/jstil/.local/lib/python3.10/site-packages/transformers/models/deberta/modeling_tf_deberta.py:123: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/distributions/bernoulli.py:86: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_deberta_for_sequence_classification_1/deberta/embeddings/word_embeddings/weight:0', 'tf_deberta_for_sequence_classification_1/deberta/embeddings/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/embeddings/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/rel_embeddings.weight:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/output/LayerNorm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_deberta_for_sequence_classification_1/deberta/embeddings/word_embeddings/weight:0', 'tf_deberta_for_sequence_classification_1/deberta/embeddings/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/embeddings/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/rel_embeddings.weight:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/output/dense/kernel:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/output/dense/bias:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification_1/deberta/encoder/layer_._11/output/LayerNorm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587/587 [==============================] - 102s 150ms/step - loss: 1.0211 - val_loss: 0.6945\n",
      "Epoch 2/10\n",
      "587/587 [==============================] - 85s 146ms/step - loss: 0.7852 - val_loss: 0.7001\n",
      "147/147 [==============================] - 9s 62ms/step - loss: 0.6945\n",
      "147/147 [==============================] - 14s 61ms/step\n",
      "y=[0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1]\n",
      "y^=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "prec=[0.0]\n",
      "rec=[0.0]\n",
      "f1=[0.0]\n",
      "f1=[0.5102040816326531]\n",
      "f1=[0.3447324875896305]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'classifier', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_deberta_for_sequence_classification/deberta/embeddings/word_embeddings/weight:0', 'tf_deberta_for_sequence_classification/deberta/embeddings/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/embeddings/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/rel_embeddings.weight:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/output/LayerNorm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_deberta_for_sequence_classification/deberta/embeddings/word_embeddings/weight:0', 'tf_deberta_for_sequence_classification/deberta/embeddings/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/embeddings/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/rel_embeddings.weight:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/self/q_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/self/v_bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/self/in_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/self/pos_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/self/pos_q_proj/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/self/pos_q_proj/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/output/dense/kernel:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/output/dense/bias:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_deberta_for_sequence_classification/deberta/encoder/layer_._11/output/LayerNorm/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/587 [==============>...............] - ETA: 36s - loss: 4.8027"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# explore\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m measure(\u001b[43mrun_model_5fold\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msg2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_sg2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeberta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m, results)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "Cell \u001b[0;32mIn [10], line 84\u001b[0m, in \u001b[0;36mrun_model_5fold\u001b[0;34m(df_name, df_train, model_name, freeze_encoder, pretrained, plot, batch_size, epochs)\u001b[0m\n\u001b[1;32m     81\u001b[0m callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# fit it\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# plot history\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# explore\n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'deberta', freeze_encoder=True, pretrained=True, plot=False, batch_size=1, epochs=10), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d34f6",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4fb65b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 17:40:10.127552: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - ETA: 0s - loss: 1.3657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 17:40:56.033725: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 48s 334ms/step - loss: 1.3657 - val_loss: 0.6988\n",
      "26/26 [==============================] - 3s 96ms/step - loss: 0.6988\n",
      "26/26 [==============================] - 5s 90ms/step\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 48s 332ms/step - loss: 1.5060 - val_loss: 0.8601\n",
      "26/26 [==============================] - 3s 112ms/step - loss: 0.8601\n",
      "26/26 [==============================] - 5s 107ms/step\n",
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 46s 316ms/step - loss: 0.9388 - val_loss: 0.6406\n",
      "26/26 [==============================] - 2s 92ms/step - loss: 0.6406\n",
      "26/26 [==============================] - 5s 93ms/step\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 46s 318ms/step - loss: 0.8912 - val_loss: 0.5908\n",
      "26/26 [==============================] - 2s 94ms/step - loss: 0.5908\n",
      "26/26 [==============================] - 5s 94ms/step\n",
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 47s 328ms/step - loss: 0.7692 - val_loss: 0.6326\n",
      "26/26 [==============================] - 3s 114ms/step - loss: 0.6326\n",
      "26/26 [==============================] - 5s 112ms/step\n",
      "{'Dataset': 'sg1', 'Model': 'bert', 'Distant': False, 'Loss': 0.6845779299736023, 'Accuracy': 0.5057459024950413, 'Precision': 0.4486722050757875, 'Recall': 0.6606263982102909, 'F1': 0.5053876906598085, 'F1 Micro': 0.5057459024950413, 'F1 Weighted': 0.4204947460977285}\n",
      "  Dataset Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n",
      "0     sg1  bert   False  0.684578  0.505746   0.448672  0.660626  0.505388   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.505746     0.420495  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29352/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'bert', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e445144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 85s 293ms/step - loss: 0.6929 - val_loss: 0.6105\n",
      "62/62 [==============================] - 6s 90ms/step - loss: 0.6105\n",
      "62/62 [==============================] - 8s 91ms/step\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 87s 306ms/step - loss: 0.7389 - val_loss: 0.6848\n",
      "62/62 [==============================] - 7s 105ms/step - loss: 0.6848\n",
      "62/62 [==============================] - 9s 102ms/step\n",
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 93s 325ms/step - loss: 1.2743 - val_loss: 0.5654\n",
      "62/62 [==============================] - 7s 107ms/step - loss: 0.5654\n",
      "62/62 [==============================] - 9s 103ms/step\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 94s 331ms/step - loss: 0.8433 - val_loss: 0.5058\n",
      "62/62 [==============================] - 7s 119ms/step - loss: 0.5058\n",
      "62/62 [==============================] - 9s 113ms/step\n",
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 89s 312ms/step - loss: 1.0630 - val_loss: 4.1868\n",
      "62/62 [==============================] - 8s 127ms/step - loss: 4.1868\n",
      "62/62 [==============================] - 10s 123ms/step\n",
      "{'Dataset': 'sg2', 'Model': 'bert', 'Distant': False, 'Loss': 1.3106484293937684, 'Accuracy': 0.5395903538527127, 'Precision': 0.5476513882959688, 'Recall': 0.9143646408839778, 'F1': 0.6639547979611393, 'F1 Micro': 0.5395903538527127, 'F1 Weighted': 0.4147783041142715}\n",
      "  Dataset Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n",
      "0     sg1  bert   False  0.684578  0.505746   0.448672  0.660626  0.505388   \n",
      "1     sg2  bert   False  1.310648  0.539590   0.547651  0.914365  0.663955   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.505746     0.420495  \n",
      "1  0.539590     0.414778  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29352/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_29352/70255357.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'bert', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d083b",
   "metadata": {},
   "source": [
    "## Bert w/ Distant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d557ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 46s 317ms/step - loss: 4.0782 - val_loss: 0.7518\n",
      "26/26 [==============================] - 2s 90ms/step - loss: 0.7518\n",
      "26/26 [==============================] - 5s 94ms/step\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 45s 317ms/step - loss: 0.8701 - val_loss: 0.5874\n",
      "26/26 [==============================] - 3s 109ms/step - loss: 0.5874\n",
      "26/26 [==============================] - 6s 108ms/step\n",
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 45s 314ms/step - loss: 0.7089 - val_loss: 0.6470\n",
      "26/26 [==============================] - 2s 90ms/step - loss: 0.6470\n",
      "26/26 [==============================] - 5s 91ms/step\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 53s 330ms/step - loss: 7.4444 - val_loss: 7.4379\n",
      "26/26 [==============================] - 3s 94ms/step - loss: 7.4379\n",
      "26/26 [==============================] - 5s 93ms/step\n",
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 47s 319ms/step - loss: 3.9527 - val_loss: 0.7807\n",
      "26/26 [==============================] - 3s 109ms/step - loss: 0.7807\n",
      "26/26 [==============================] - 5s 110ms/step\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "{'Dataset': 'sg1', 'Model': 'bert', 'Distant': True, 'Loss': 2.0409685611724853, 'Accuracy': 0.5045411838396492, 'Precision': 0.3954679311867356, 'Recall': 0.7543624161073825, 'F1': 0.5177566906678607, 'F1 Micro': 0.5045411838396492, 'F1 Weighted': 0.37350068053963575}\n",
      "  Dataset Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n",
      "0     sg1  bert   False  0.684578  0.505746   0.448672  0.660626  0.505388   \n",
      "1     sg2  bert   False  1.310648  0.539590   0.547651  0.914365  0.663955   \n",
      "2     sg1  bert    True  2.040969  0.504541   0.395468  0.754362  0.517757   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.505746     0.420495  \n",
      "1  0.539590     0.414778  \n",
      "2  0.504541     0.373501  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29352/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_29352/70255357.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'bert', freeze_encoder=False, pretrained=True, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f6650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 18:34:07.967670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 18:34:08.020034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 18:34:08.020585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 18:34:08.021728: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-04 18:34:08.024427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 18:34:08.024985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 18:34:08.025458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 18:34:09.781679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 18:34:09.782760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 18:34:09.782787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-04 18:34:09.783493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 18:34:09.783723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 87s 296ms/step - loss: 0.7730 - val_loss: 0.7206\n",
      "62/62 [==============================] - 6s 92ms/step - loss: 0.7206\n",
      "62/62 [==============================] - 8s 92ms/step\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 88s 308ms/step - loss: 3.3643 - val_loss: 0.6133\n",
      "62/62 [==============================] - 7s 107ms/step - loss: 0.6133\n",
      "62/62 [==============================] - 9s 105ms/step\n",
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 92s 324ms/step - loss: 4.1054 - val_loss: 4.0479\n",
      "62/62 [==============================] - 7s 109ms/step - loss: 4.0479\n",
      "62/62 [==============================] - 9s 101ms/step\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 94s 331ms/step - loss: 0.6372 - val_loss: 0.6650\n",
      "62/62 [==============================] - 7s 118ms/step - loss: 0.6650\n",
      "62/62 [==============================] - 10s 116ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 89s 310ms/step - loss: 0.8327 - val_loss: 0.6927\n",
      "62/62 [==============================] - 8s 126ms/step - loss: 0.6927\n",
      "62/62 [==============================] - 10s 122ms/step\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "{'Dataset': 'sg2', 'Model': 'bert', 'Distant': True, 'Loss': 1.3478875994682311, 'Accuracy': 0.4408014977108009, 'Precision': 0.30427887183426255, 'Recall': 0.44972375690607735, 'F1': 0.3280262398556598, 'F1 Micro': 0.4408014977108009, 'F1 Weighted': 0.3241086958680499}\n",
      "  Dataset Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n",
      "0     sg2  bert    True  1.347888  0.440801   0.304279  0.449724  0.328026   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.440801     0.324109  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31725/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'bert', freeze_encoder=False, pretrained=True, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f92b4",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8420cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 48s 319ms/step - loss: 0.8301 - val_loss: 0.6804\n",
      "26/26 [==============================] - 2s 89ms/step - loss: 0.6804\n",
      "26/26 [==============================] - 5s 89ms/step\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 46s 324ms/step - loss: 0.8909 - val_loss: 0.6768\n",
      "26/26 [==============================] - 3s 108ms/step - loss: 0.6768\n",
      "26/26 [==============================] - 5s 110ms/step\n",
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 46s 320ms/step - loss: 0.8623 - val_loss: 1.1006\n",
      "26/26 [==============================] - 2s 93ms/step - loss: 1.1006\n",
      "26/26 [==============================] - 5s 92ms/step\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 46s 324ms/step - loss: 0.7796 - val_loss: 0.5541\n",
      "26/26 [==============================] - 2s 92ms/step - loss: 0.5541\n",
      "26/26 [==============================] - 6s 92ms/step\n",
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 46s 326ms/step - loss: 1.3416 - val_loss: 0.9448\n",
      "26/26 [==============================] - 3s 111ms/step - loss: 0.9448\n",
      "26/26 [==============================] - 5s 111ms/step\n",
      "{'Dataset': 'sg1', 'Model': 'roberta', 'Distant': False, 'Loss': 0.7913413882255554, 'Accuracy': 0.5194070362250758, 'Precision': 0.2197734627831715, 'Recall': 0.24966442953020135, 'F1': 0.20094440149599882, 'F1 Micro': 0.5194070362250758, 'F1 Weighted': 0.37787281124846717}\n",
      "  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n",
      "0     sg2     bert    True  1.347888  0.440801   0.304279  0.449724  0.328026   \n",
      "1     sg1  roberta   False  0.791341  0.519407   0.219773  0.249664  0.200944   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.440801     0.324109  \n",
      "1  0.519407     0.377873  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/tmp/ipykernel_31725/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_31725/70255357.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'roberta', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0a6aa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 93s 326ms/step - loss: 0.8962 - val_loss: 0.7187\n",
      "62/62 [==============================] - 7s 112ms/step - loss: 0.7187\n",
      "62/62 [==============================] - 9s 109ms/step\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 94s 331ms/step - loss: 0.7479 - val_loss: 0.7447\n",
      "62/62 [==============================] - 7s 106ms/step - loss: 0.7447\n",
      "62/62 [==============================] - 9s 101ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 95s 335ms/step - loss: 0.8058 - val_loss: 0.6521\n",
      "62/62 [==============================] - 7s 114ms/step - loss: 0.6521\n",
      "62/62 [==============================] - 10s 106ms/step\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 95s 337ms/step - loss: 0.6882 - val_loss: 0.6090\n",
      "62/62 [==============================] - 7s 117ms/step - loss: 0.6090\n",
      "62/62 [==============================] - 9s 108ms/step\n",
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 91s 316ms/step - loss: 0.7455 - val_loss: 0.7118\n",
      "62/62 [==============================] - 8s 126ms/step - loss: 0.7118\n",
      "62/62 [==============================] - 10s 118ms/step\n",
      "{'Dataset': 'sg2', 'Model': 'roberta', 'Distant': False, 'Loss': 0.6872531771659851, 'Accuracy': 0.5096250162190217, 'Precision': 0.27268651024456136, 'Recall': 0.37071823204419896, 'F1': 0.3004246529808211, 'F1 Micro': 0.5096250162190217, 'F1 Weighted': 0.4013159745746312}\n",
      "  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n",
      "0     sg2     bert    True  1.347888  0.440801   0.304279  0.449724  0.328026   \n",
      "1     sg1  roberta   False  0.791341  0.519407   0.219773  0.249664  0.200944   \n",
      "2     sg2  roberta   False  0.687253  0.509625   0.272687  0.370718  0.300425   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.440801     0.324109  \n",
      "1  0.519407     0.377873  \n",
      "2  0.509625     0.401316  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/tmp/ipykernel_31725/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_31725/70255357.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'roberta', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd8dcca",
   "metadata": {},
   "source": [
    "## Roberta w/ Distant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aacaf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 19:40:24.572872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 19:40:24.593429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 19:40:24.593968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 19:40:24.594820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-04 19:40:24.596673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 19:40:24.597137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 19:40:24.597583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 19:40:25.491660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 19:40:25.492677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 19:40:25.492706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-04 19:40:25.493468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 19:40:25.493585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 76s 150ms/step - loss: 0.7609 - val_loss: 0.6955\n",
      "104/104 [==============================] - 7s 67ms/step - loss: 0.6955\n",
      "104/104 [==============================] - 9s 63ms/step\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 77s 155ms/step - loss: 2.6149 - val_loss: 4.0653\n",
      "103/103 [==============================] - 7s 65ms/step - loss: 4.0653\n",
      "103/103 [==============================] - 9s 68ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 78s 158ms/step - loss: 0.8305 - val_loss: 0.7328\n",
      "103/103 [==============================] - 7s 64ms/step - loss: 0.7328\n",
      "103/103 [==============================] - 9s 65ms/step\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 78s 158ms/step - loss: 1.4463 - val_loss: 0.7727\n",
      "103/103 [==============================] - 7s 68ms/step - loss: 0.7727\n",
      "103/103 [==============================] - 10s 64ms/step\n",
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 78s 157ms/step - loss: 0.7731 - val_loss: 0.7667\n",
      "103/103 [==============================] - 7s 67ms/step - loss: 0.7667\n",
      "103/103 [==============================] - 9s 64ms/step\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "{'Dataset': 'sg1', 'Model': 'roberta', 'Distant': True, 'Loss': 1.406602418422699, 'Accuracy': 0.49545881616035076, 'Precision': 0.28898415500357244, 'Recall': 0.5986577181208054, 'F1': 0.3898024901340621, 'F1 Micro': 0.49545881616035076, 'F1 Weighted': 0.3289035999085809}\n",
      "  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n",
      "0     sg1  roberta    True  1.406602  0.495459   0.288984  0.598658  0.389802   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.495459     0.328904  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2014/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'roberta', freeze_encoder=False, pretrained=True, plot=False, batch_size=3), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53bd4247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 20:12:03.190776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 20:12:03.191435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 20:12:03.191982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 20:12:03.192703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 20:12:03.192717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-04 20:12:03.193207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 20:12:03.193247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2938/2938 [==============================] - 387s 127ms/step - loss: 7.1630 - val_loss: 7.5970\n",
      "735/735 [==============================] - 47s 64ms/step - loss: 7.5970\n",
      "735/735 [==============================] - 45s 58ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2938/2938 [==============================] - 379s 125ms/step - loss: 3.9510 - val_loss: 4.1579\n",
      "735/735 [==============================] - 46s 63ms/step - loss: 4.1579\n",
      "735/735 [==============================] - 46s 59ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2938/2938 [==============================] - 381s 125ms/step - loss: 3.4377 - val_loss: 4.1479\n",
      "735/735 [==============================] - 46s 62ms/step - loss: 4.1479\n",
      "735/735 [==============================] - 48s 62ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2939/2939 [==============================] - 396s 130ms/step - loss: 7.5997 - val_loss: 7.6074\n",
      "734/734 [==============================] - 46s 62ms/step - loss: 7.6074\n",
      "734/734 [==============================] - 48s 63ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2939/2939 [==============================] - 377s 124ms/step - loss: 7.5998 - val_loss: 7.6074\n",
      "734/734 [==============================] - 45s 62ms/step - loss: 7.6074\n",
      "734/734 [==============================] - 46s 59ms/step\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "{'Dataset': 'sg2', 'Model': 'roberta', 'Distant': True, 'Loss': 6.223529911041259, 'Accuracy': 0.5044897959183674, 'Precision': 0.09863760217983651, 'Recall': 0.2, 'F1': 0.13211678832116788, 'F1 Micro': 0.5044897959183674, 'F1 Weighted': 0.33835280553931646}\n",
      "  Dataset    Model Distant     Loss  Accuracy  Precision  Recall        F1  \\\n",
      "0     sg2  roberta    True  6.22353   0.50449   0.098638     0.2  0.132117   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0   0.50449     0.338353  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3499/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'roberta', freeze_encoder=False, pretrained=True, plot=False, batch_size=1), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220cd051",
   "metadata": {},
   "source": [
    "## Deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a684ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "2022-11-04 21:01:26.354003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 21:01:26.354872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 21:01:26.355639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 21:01:26.356675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 21:01:26.356705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-04 21:01:26.357201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-04 21:01:26.357245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jstil/.local/lib/python3.10/site-packages/transformers/models/deberta/modeling_tf_deberta.py:123: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/distributions/bernoulli.py:86: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "103/103 [==============================] - 93s 617ms/step - loss: 0.9657 - val_loss: 4.2263\n",
      "26/26 [==============================] - 3s 107ms/step - loss: 4.2263\n",
      "26/26 [==============================] - 8s 103ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 91s 619ms/step - loss: 7.4527 - val_loss: 7.8960\n",
      "26/26 [==============================] - 3s 126ms/step - loss: 7.8960\n",
      "26/26 [==============================] - 9s 124ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 89s 604ms/step - loss: 7.2480 - val_loss: 7.4379\n",
      "26/26 [==============================] - 3s 105ms/step - loss: 7.4379\n",
      "26/26 [==============================] - 8s 104ms/step\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 90s 608ms/step - loss: 7.4444 - val_loss: 7.4379\n",
      "26/26 [==============================] - 3s 107ms/step - loss: 7.4379\n",
      "26/26 [==============================] - 8s 105ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 92s 625ms/step - loss: 3.0622 - val_loss: 0.6837\n",
      "26/26 [==============================] - 3s 128ms/step - loss: 0.6837\n",
      "26/26 [==============================] - 10s 128ms/step\n",
      "{'Dataset': 'sg1', 'Model': 'deberta', 'Distant': False, 'Loss': 5.5363799929618835, 'Accuracy': 0.5032258064516129, 'Precision': 0.19288025889967636, 'Recall': 0.4, 'F1': 0.26026200873362443, 'F1 Micro': 0.5032258064516129, 'F1 Weighted': 0.3370982826616396}\n",
      "  Dataset    Model Distant     Loss  Accuracy  Precision  Recall        F1  \\\n",
      "0     sg1  deberta   False  5.53638  0.503226    0.19288     0.4  0.260262   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.503226     0.337098  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4205/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'deberta', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02bb2a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 174s 594ms/step - loss: 1.5359 - val_loss: 0.6792\n",
      "62/62 [==============================] - 8s 127ms/step - loss: 0.6792\n",
      "62/62 [==============================] - 13s 120ms/step\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 173s 600ms/step - loss: 2.4936 - val_loss: 0.7449\n",
      "62/62 [==============================] - 8s 126ms/step - loss: 0.7449\n",
      "62/62 [==============================] - 14s 113ms/step\n",
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 174s 602ms/step - loss: 4.1881 - val_loss: 4.1493\n",
      "62/62 [==============================] - 8s 132ms/step - loss: 4.1493\n",
      "62/62 [==============================] - 12s 116ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 173s 594ms/step - loss: 7.4869 - val_loss: 7.7285\n",
      "62/62 [==============================] - 8s 130ms/step - loss: 7.7285\n",
      "62/62 [==============================] - 13s 124ms/step\n",
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 163s 549ms/step - loss: 3.6388 - val_loss: 7.6679\n",
      "62/62 [==============================] - 9s 142ms/step - loss: 7.6679\n",
      "62/62 [==============================] - 15s 130ms/step\n",
      "{'Dataset': 'sg2', 'Model': 'deberta', 'Distant': False, 'Loss': 4.193958270549774, 'Accuracy': 0.4941496598639456, 'Precision': 0.19331386117264227, 'Recall': 0.381767955801105, 'F1': 0.25662009957944604, 'F1 Micro': 0.4941496598639456, 'F1 Weighted': 0.33465556742634206}\n",
      "  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n",
      "0     sg1  deberta   False  5.536380  0.503226   0.192880  0.400000  0.260262   \n",
      "1     sg2  deberta   False  4.193958  0.494150   0.193314  0.381768  0.256620   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.503226     0.337098  \n",
      "1  0.494150     0.334656  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/tmp/ipykernel_4205/70255357.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_4205/70255357.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'deberta', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeca876",
   "metadata": {},
   "source": [
    "## Deberta w/ Distant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bb34cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 11:04:44.627661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 11:04:44.628237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 11:04:44.628746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 11:04:44.629530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 11:04:44.629544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-06 11:04:44.630045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 11:04:44.630073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'classifier', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'classifier', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jstil/.local/lib/python3.10/site-packages/transformers/models/deberta/modeling_tf_deberta.py:123: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/distributions/bernoulli.py:86: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "1236/1236 [==============================] - 330s 244ms/step - loss: 7.7775 - val_loss: 7.8706\n",
      "310/310 [==============================] - 19s 62ms/step - loss: 7.8706\n",
      "310/310 [==============================] - 24s 62ms/step\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'classifier', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237/1237 [==============================] - 325s 242ms/step - loss: 1.6887 - val_loss: 0.6949\n",
      "309/309 [==============================] - 20s 64ms/step - loss: 0.6949\n",
      "309/309 [==============================] - 25s 63ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'classifier', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237/1237 [==============================] - 324s 241ms/step - loss: 7.0092 - val_loss: 7.4379\n",
      "309/309 [==============================] - 19s 62ms/step - loss: 7.4379\n",
      "309/309 [==============================] - 24s 60ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'classifier', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237/1237 [==============================] - 326s 241ms/step - loss: 4.4080 - val_loss: 4.1017\n",
      "309/309 [==============================] - 20s 64ms/step - loss: 4.1017\n",
      "309/309 [==============================] - 24s 61ms/step\n",
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['cls_dropout', 'classifier', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237/1237 [==============================] - 324s 241ms/step - loss: 4.4301 - val_loss: 4.0913\n",
      "309/309 [==============================] - 20s 64ms/step - loss: 4.0913\n",
      "309/309 [==============================] - 25s 64ms/step\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "  Dataset    Model Distant      Loss  Accuracy  Precision  Recall        F1  \\\n",
      "0     sg1  deberta    True  4.839279  0.496774   0.289654     0.6  0.390697   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.496774      0.32993  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1504/1019128501.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'deberta', freeze_encoder=False, pretrained=True, plot=False, batch_size=1), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4c176ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 11:51:47.407862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 11:51:47.408474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 11:51:47.409021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 11:51:47.409859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 11:51:47.409873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-06 11:51:47.410356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 11:51:47.410380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jstil/.local/lib/python3.10/site-packages/transformers/models/deberta/modeling_tf_deberta.py:123: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/distributions/bernoulli.py:86: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2938/2938 [==============================] - 738s 241ms/step - loss: 1.4103 - val_loss: 0.7042\n",
      "735/735 [==============================] - 46s 63ms/step - loss: 0.7042\n",
      "735/735 [==============================] - 50s 62ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2938/2938 [==============================] - 733s 241ms/step - loss: 2.1964 - val_loss: 0.6956\n",
      "735/735 [==============================] - 46s 62ms/step - loss: 0.6956\n",
      "735/735 [==============================] - 50s 62ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2938/2938 [==============================] - 735s 241ms/step - loss: 2.6477 - val_loss: 0.7062\n",
      "735/735 [==============================] - 46s 63ms/step - loss: 0.7062\n",
      "735/735 [==============================] - 51s 62ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2939/2939 [==============================] - 728s 239ms/step - loss: 1.4574 - val_loss: 0.6990\n",
      "734/734 [==============================] - 46s 63ms/step - loss: 0.6990\n",
      "734/734 [==============================] - 50s 61ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2939/2939 [==============================] - 730s 240ms/step - loss: 2.4102 - val_loss: 0.6983\n",
      "734/734 [==============================] - 46s 63ms/step - loss: 0.6983\n",
      "734/734 [==============================] - 50s 62ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "  Dataset    Model Distant      Loss  Accuracy  Precision  Recall   F1  \\\n",
      "0     sg2  deberta    True  0.700658  0.507215        0.0     0.0  0.0   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0  0.507215      0.34138  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2358/1019128501.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'deberta', freeze_encoder=False, pretrained=True, plot=False, batch_size=1), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318ca89",
   "metadata": {},
   "source": [
    "## Distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3883467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 15:29:10.286044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 15:29:10.286707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 15:29:10.287264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 15:29:10.288087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 15:29:10.288101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-06 15:29:10.288624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 15:29:10.288653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 24s 157ms/step - loss: 0.6755 - val_loss: 0.6323\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 0.6323\n",
      "26/26 [==============================] - 2s 42ms/step\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 23s 160ms/step - loss: 0.6761 - val_loss: 0.5631\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.5631\n",
      "26/26 [==============================] - 2s 52ms/step\n",
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 23s 159ms/step - loss: 0.7082 - val_loss: 0.5871\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 0.5871\n",
      "26/26 [==============================] - 2s 42ms/step\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 23s 158ms/step - loss: 0.6686 - val_loss: 0.5788\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 0.5788\n",
      "26/26 [==============================] - 2s 43ms/step\n",
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 23s 161ms/step - loss: 0.7069 - val_loss: 0.6872\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.6872\n",
      "26/26 [==============================] - 2s 53ms/step\n",
      "  Dataset       Model Distant      Loss  Accuracy  Precision    Recall  \\\n",
      "0     sg1  distilbert   False  0.609679   0.47928   0.459436  0.707409   \n",
      "\n",
      "         F1  F1 Micro  F1 Weighted  \n",
      "0  0.552116   0.47928     0.440871  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8976/1019128501.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'distilbert', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f067395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 42s 148ms/step - loss: 4.1420 - val_loss: 4.0909\n",
      "62/62 [==============================] - 3s 45ms/step - loss: 4.0909\n",
      "62/62 [==============================] - 4s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 44s 152ms/step - loss: 0.6529 - val_loss: 0.6279\n",
      "62/62 [==============================] - 3s 48ms/step - loss: 0.6279\n",
      "62/62 [==============================] - 4s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 44s 154ms/step - loss: 0.6475 - val_loss: 0.5870\n",
      "62/62 [==============================] - 3s 48ms/step - loss: 0.5870\n",
      "62/62 [==============================] - 4s 46ms/step\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 46s 162ms/step - loss: 0.6424 - val_loss: 0.6540\n",
      "62/62 [==============================] - 4s 58ms/step - loss: 0.6540\n",
      "62/62 [==============================] - 5s 56ms/step\n",
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 44s 156ms/step - loss: 0.6471 - val_loss: 0.6136\n",
      "62/62 [==============================] - 4s 63ms/step - loss: 0.6136\n",
      "62/62 [==============================] - 5s 60ms/step\n",
      "  Dataset       Model Distant      Loss  Accuracy  Precision    Recall  \\\n",
      "0     sg1  distilbert   False  0.609679  0.479280   0.459436  0.707409   \n",
      "1     sg1       xlnet   False  2.144441  0.492943   0.334361  0.265423   \n",
      "2     sg1     electra   False  0.690808  0.481902   0.461244  0.320394   \n",
      "3     sg2  distilbert   False  1.314664  0.555149   0.311605  0.349171   \n",
      "\n",
      "         F1  F1 Micro  F1 Weighted  \n",
      "0  0.552116  0.479280     0.440871  \n",
      "1  0.216017  0.492943     0.367079  \n",
      "2  0.341885  0.481902     0.441015  \n",
      "3  0.286102  0.555149     0.445587  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8976/1019128501.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_8976/1019128501.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'distilbert', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920adc2e",
   "metadata": {},
   "source": [
    "## Xlnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4e5a107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "103/103 [==============================] - 50s 356ms/step - loss: 1.0473 - val_loss: 0.5797\n",
      "26/26 [==============================] - 2s 91ms/step - loss: 0.5797\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "104/104 [==============================] - 48s 356ms/step - loss: 0.8895 - val_loss: 0.6973\n",
      "26/26 [==============================] - 3s 113ms/step - loss: 0.6973\n",
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "104/104 [==============================] - 49s 353ms/step - loss: 1.0760 - val_loss: 4.7514\n",
      "26/26 [==============================] - 2s 90ms/step - loss: 4.7514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "104/104 [==============================] - 49s 356ms/step - loss: 4.1434 - val_loss: 4.0667\n",
      "26/26 [==============================] - 2s 93ms/step - loss: 4.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "104/104 [==============================] - 48s 357ms/step - loss: 1.0787 - val_loss: 0.6270\n",
      "26/26 [==============================] - 3s 113ms/step - loss: 0.6270\n",
      "  Dataset       Model Distant      Loss  Accuracy  Precision    Recall  \\\n",
      "0     sg1  distilbert   False  0.609679  0.479280   0.459436  0.707409   \n",
      "1     sg1       xlnet   False  2.144441  0.492943   0.334361  0.265423   \n",
      "\n",
      "         F1  F1 Micro  F1 Weighted  \n",
      "0  0.552116  0.479280     0.440871  \n",
      "1  0.216017  0.492943     0.367079  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8976/1019128501.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_8976/1019128501.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'xlnet', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d64444ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "245/245 [==============================] - 98s 354ms/step - loss: 0.9909 - val_loss: 0.7380\n",
      "62/62 [==============================] - 6s 102ms/step - loss: 0.7380\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "245/245 [==============================] - 95s 344ms/step - loss: 1.4657 - val_loss: 0.7030\n",
      "62/62 [==============================] - 7s 106ms/step - loss: 0.7030\n",
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "245/245 [==============================] - 95s 342ms/step - loss: 0.9387 - val_loss: 1.1227\n",
      "62/62 [==============================] - 6s 97ms/step - loss: 1.1227\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "245/245 [==============================] - 96s 346ms/step - loss: 1.6078 - val_loss: 0.7464\n",
      "62/62 [==============================] - 7s 105ms/step - loss: 0.7464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstil/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/jstil/miniconda3/envs/tf/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "245/245 [==============================] - 90s 324ms/step - loss: 1.1332 - val_loss: 0.6890\n",
      "62/62 [==============================] - 7s 114ms/step - loss: 0.6890\n",
      "  Dataset       Model Distant      Loss  Accuracy  Precision    Recall  \\\n",
      "0     sg1  distilbert   False  0.609679  0.479280   0.459436  0.707409   \n",
      "1     sg1       xlnet   False  2.144441  0.492943   0.334361  0.265423   \n",
      "2     sg1     electra   False  0.690808  0.481902   0.461244  0.320394   \n",
      "3     sg2  distilbert   False  1.314664  0.555149   0.311605  0.349171   \n",
      "4     sg2       xlnet   False  0.799817  0.453601   0.313545  0.506077   \n",
      "\n",
      "         F1  F1 Micro  F1 Weighted  \n",
      "0  0.552116  0.479280     0.440871  \n",
      "1  0.216017  0.492943     0.367079  \n",
      "2  0.341885  0.481902     0.441015  \n",
      "3  0.286102  0.555149     0.445587  \n",
      "4  0.355963  0.453601     0.320916  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8976/1019128501.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_8976/1019128501.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'xlnet', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477bc36",
   "metadata": {},
   "source": [
    "## Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02019c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 27s 134ms/step - loss: 0.9112 - val_loss: 0.6850\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 0.6850\n",
      "26/26 [==============================] - 4s 63ms/step\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 27s 140ms/step - loss: 0.8484 - val_loss: 0.6754\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 0.6754\n",
      "26/26 [==============================] - 4s 61ms/step\n",
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 27s 146ms/step - loss: 0.7647 - val_loss: 0.6701\n",
      "26/26 [==============================] - 2s 57ms/step - loss: 0.6701\n",
      "26/26 [==============================] - 4s 60ms/step\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 27s 139ms/step - loss: 0.8209 - val_loss: 0.6898\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.6898\n",
      "26/26 [==============================] - 4s 63ms/step\n",
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 27s 146ms/step - loss: 1.1027 - val_loss: 0.7337\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 0.7337\n",
      "26/26 [==============================] - 4s 67ms/step\n",
      "  Dataset       Model Distant      Loss  Accuracy  Precision    Recall  \\\n",
      "0     sg1  distilbert   False  0.609679  0.479280   0.459436  0.707409   \n",
      "1     sg1       xlnet   False  2.144441  0.492943   0.334361  0.265423   \n",
      "2     sg1     electra   False  0.690808  0.481902   0.461244  0.320394   \n",
      "\n",
      "         F1  F1 Micro  F1 Weighted  \n",
      "0  0.552116  0.479280     0.440871  \n",
      "1  0.216017  0.492943     0.367079  \n",
      "2  0.341885  0.481902     0.441015  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8976/1019128501.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_8976/1019128501.py:23: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg1 \n",
    "results = measure(run_model_5fold('sg1', df_sg1, 'electra', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8073b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 16:27:39.974889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:39.975541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:39.976120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:39.976872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:39.976886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-06 16:27:39.977408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-06 16:27:39.977455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 46s 127ms/step - loss: 0.8592 - val_loss: 0.6697\n",
      "62/62 [==============================] - 4s 57ms/step - loss: 0.6697\n",
      "62/62 [==============================] - 6s 59ms/step\n",
      "### Start fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 43s 127ms/step - loss: 0.7598 - val_loss: 0.5299\n",
      "62/62 [==============================] - 4s 62ms/step - loss: 0.5299\n",
      "62/62 [==============================] - 6s 62ms/step\n",
      "### Start fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 43s 125ms/step - loss: 0.8890 - val_loss: 0.5780\n",
      "62/62 [==============================] - 4s 58ms/step - loss: 0.5780\n",
      "62/62 [==============================] - 6s 57ms/step\n",
      "### Start fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 42s 123ms/step - loss: 7.5996 - val_loss: 7.6074\n",
      "62/62 [==============================] - 4s 60ms/step - loss: 7.6074\n",
      "62/62 [==============================] - 6s 60ms/step\n",
      "### Start fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 42s 121ms/step - loss: 0.7999 - val_loss: 0.6087\n",
      "62/62 [==============================] - 4s 57ms/step - loss: 0.6087\n",
      "62/62 [==============================] - 6s 60ms/step\n",
      "  Dataset    Model Distant      Loss  Accuracy  Precision   Recall        F1  \\\n",
      "0     sg2  electra   False  1.998728   0.52462    0.53334  0.40884  0.414638   \n",
      "\n",
      "   F1 Micro  F1 Weighted  \n",
      "0   0.52462     0.463769  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12368/1019128501.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Sg2 \n",
    "results = measure(run_model_5fold('sg2', df_sg2, 'electra', freeze_encoder=False, pretrained=False, plot=False), results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872bcb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
